# BluePy - AI Conversational Interface for ARGO Data

An intelligent conversational interface for querying and visualizing ARGO oceanographic float data using RAG (Retrieval-Augmented Generation) and MCP (Model Context Protocol).

## Architecture

**Data Flow:** Ingest Argo NetCDF â†’ normalize & store (Postgres + Parquet) â†’ index metadata & embeddings (FAISS/Chroma) â†’ RAG + MCP translator (LLM) â†’ Backend APIs â†’ Interactive dashboard + Chat UI (Streamlit) + visualizations (Plotly/Leaflet)
![Alt text for accessibility](bluepy_rag_arch.png)

## Features

- ğŸŒŠ **ARGO Data Ingestion**: Parse NetCDF files and normalize to structured formats
- ğŸ—„ï¸ **Dual Storage**: PostgreSQL with PostGIS for spatial queries + Parquet for analytics
- ğŸ” **Vector Search**: FAISS/Chroma for semantic retrieval of profiles and metadata
- ğŸ¤– **RAG + MCP**: LLM-powered natural language to SQL translation with structured outputs
- ğŸš€ **FastAPI Backend**: RESTful APIs for chat, queries, and data access
- ğŸ“Š **Interactive Frontend**: Streamlit dashboard with chat, maps, and visualizations
- ğŸ—ºï¸ **Geospatial Viz**: Leaflet maps for float trajectories, Plotly for profiles

## Project Structure

```
bluepy/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ db/               # Database models, schema, connections
â”‚   â”œâ”€â”€ rag/              # RAG pipeline + MCP translator
â”‚   â”œâ”€â”€ main.py           # FastAPI app entry point
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py            # Streamlit main app (Premium UI)
â”‚   â”œâ”€â”€ styles.py         # Custom CSS (Glass morphism theme)
â”‚   â””â”€â”€ __pycache__/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ parsers/          # NetCDF file parsers
â”‚   â”œâ”€â”€ pipeline.py       # Data ingestion orchestration
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ sample_data_generator.py  # Generate realistic ARGO data
â”‚   â”œâ”€â”€ init_db.py                # Database initialization
â”‚   â”œâ”€â”€ embed_profiles.py         # Generate embeddings
â”‚   â”œâ”€â”€ run_all.bat/sh            # Full stack launcher
â”‚   â””â”€â”€ ...
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw NetCDF files
â”‚   â”œâ”€â”€ processed/        # Parquet files
â”‚   â””â”€â”€ embeddings/       # ChromaDB vector storage
â”œâ”€â”€ docs/                 # ğŸ“š Complete documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md
â”‚   â”œâ”€â”€ API_DOCUMENTATION.md
â”‚   â”œâ”€â”€ QUICKSTART.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ ... (12 files total)
â”œâ”€â”€ tests/                # Unit & integration tests
â”œâ”€â”€ docker/               # Dockerfile configurations
â”œâ”€â”€ setup.bat             # Complete setup script
â”œâ”€â”€ demo_frontend.bat     # Quick UI demo launcher
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ .env.example          # Environment template
â””â”€â”€ README.md
```

## Quick Start

### Prerequisites

- Python 3.10+
- PostgreSQL 14+ with PostGIS extension
- Docker (optional, for containerized deployment)

### Installation

1. Clone and navigate to the project:
```bash
cd bluepy
```

2. Create virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your Groq API key and database credentials
```

5. Generate sample data (375 realistic ARGO profiles):
```bash
python scripts/sample_data_generator.py --num-floats 15 --region all
```

6. Initialize database:
```bash
python scripts/init_db.py
```

6. Run data ingestion (example):
```bash
python ingestion/pipeline.py --input data/raw --output data/processed
```

### Running the Application

**Backend API:**
```bash
uvicorn backend.main:app --reload --port 8000
```

**Frontend Dashboard:**
```bash
streamlit run frontend/app.py --server.port 8501
```

Access the application at `http://localhost:8501`

## Usage Examples

### Natural Language Queries

- "Show me salinity profiles near the equator in March 2023"
- "What's the average temperature at 500m depth in the Indian Ocean?"
- "Find floats with anomalous oxygen levels in the last 6 months"
- "Plot temperature vs depth for float 2902123"

### API Endpoints

- `GET /` - Root endpoint
- `GET /health` - Health check
- `POST /chat` - AI conversational interface (RAG + MCP)
- `GET /profiles` - List profiles with filters
- `GET /profile/{profile_id}` - Get specific profile details
- `GET /floats` - List all floats with filters
- `GET /statistics` - Database statistics
- `GET /map/geojson` - GeoJSON data for mapping
- `DELETE /session/{session_id}` - Clear conversation session

## Configuration

Key environment variables in `.env`:

```env
# Database
DATABASE_URL=postgresql://user:pass@localhost:5432/argo_db
POSTGRES_USER=argo_user
POSTGRES_PASSWORD=secure_password
POSTGRES_DB=argo_db

# LLM Configuration (Using Groq for fast inference)
GROQ_API_KEY=your_groq_api_key_here
LLM_PROVIDER=groq
LLM_MODEL=llama-3.3-70b-versatile
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Vector DB
VECTOR_DB_TYPE=chroma  # or faiss
CHROMA_PERSIST_DIR=./data/embeddings/chroma

# API
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:8501

# Frontend
STREAMLIT_SERVER_PORT=8501
MAP_PROVIDER=leaflet
```

## Development

### Running Tests
```bash
pytest tests/ -v --cov=backend --cov=ingestion
```

### Code Quality
```bash
# Linting
flake8 backend/ ingestion/ frontend/

# Type checking
mypy backend/ ingestion/

# Formatting
black backend/ ingestion/ frontend/
```

## Deployment

### Docker Compose
```bash
docker-compose up -d
```

### Kubernetes
```bash
kubectl apply -f k8s/
```

## Data Schema

### PostgreSQL Tables

**argo_profile** - Main profile data table with spatial indexing
**argo_profile_meta** - Profile metadata and summaries
**argo_float** - Float information and trajectories

See `backend/db/schema.sql` for complete schema definitions.

## Technology Stack

- **Backend**: FastAPI, SQLAlchemy, psycopg2
- **Database**: PostgreSQL + PostGIS, Parquet (PyArrow)
- **Vector DB**: ChromaDB / FAISS
- **LLM**: OpenAI GPT-4 / Anthropic Claude
- **Frontend**: Streamlit, Plotly, Folium/Leaflet
- **Data Processing**: xarray, netCDF4, pandas, numpy
- **Deployment**: Docker, Docker Compose, Kubernetes

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

MIT License - see LICENSE file for details

## Acknowledgments

- ARGO Program for oceanographic data
- OpenAI for LLM capabilities
- Streamlit community for excellent framework

## Support

For issues and questions, please open a GitHub issue or contact the maintainers.
